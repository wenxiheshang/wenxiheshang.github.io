---
'title': '实习总结'
'date': 2018-01-04
---
# 实习总结

这个实习马上要到尽头了。需要系统总结下自己在实习期间学到的东西。现在想想，自己的这个人生中第一份真正意义上的实习，比较中规中矩。整体上还不错，符合实习的调性，大部分时候听从leader的安排，完成一些大部分都是由体力活构成的任务。不过，说真的，我感觉我的这份实习应该优于平均水平。毕竟，我还是完成了不少还算独立的任务的。

这些独立的任务，属于核心业务，难度适中，自己能够通过自己的思考和学习独立且按时完成。在这个过程中，主要涉及基本的抽象和封装以及通过数据库sql语句完成一些数据聚合、分析和处理方面的任务。

通过这三个多月的实习，我基本上初步掌握了 Python 的基本编程方法，爬虫的基本技能以及数据库查询、更新、聚合、分析和处理方面的基本技能。

爬虫方面，基本上是三块：

- 网络请求的分析，涉及chrome开发工具的使用，电脑端代理工具的使用，curl 的使用，cookie和json的获取和清洗
- 数据的抓取，通过xpath或者bs4种的dom节点查询
- selenium 的使用，表单的填写，验证码图片的截图，frame的定位，元素的定位、点击和拖动

与爬虫密切相关的，就是数据的存储和清洗：

- 时间戳的处理，各种转换
- 字符串的各种处理
- json 的各种处理和转换

以及各种异常情况的容忍处理，日志输出处理和异常报错处理

其余的，还有基本的 http server 的使用，redis 的使用，mongoDB 的使用，以及 nsq 的部署和用于搜集日志的封装处理。

对了，还有数据库流水表的使用。日志只适合寻找一小段时间内发生的异常。在需要统计某个时间跨度内的异常情况时，还是需要一个数据库流水表。

最后，还部署了几个基础设施，虽然没怎么用上大用场，但也算积累经验：

- 将 nsq 用于搜集散落在各个服务器上的日志输出
- Prometheus 搭配一些 exporter 以及 Grafana，实现数据可视化
- 利用 Flask 搭建简单的服务器，给几个部署上其他机器上的程序提供统一的服务支持

相对而言，这次的实习已经非常充实和有益了。

自己需要进一步学习和掌握的是：
- 爬虫框架的使用，scrapy 或者 pyspider
- 数据结构和算法，尤其是基本的复杂度分析和排序、二叉树算法
- 抽象和封装的理解和使用
- 数据库的基本理论和基本使用
- 小程序的基本框架和原理
- html，js 的基本框架和原理
- Go 的基本使用，字符串处理，网络通讯，数据库交互，channel 的并发使用
- Tensorflow 的实践积累

最终，自己跑到了爬虫这一块，和网络请求打交道，和数据打交道，和应用级别的成熟人工智能模型打交道。那么，自己需要掌握的基础理论就很清楚了：

- 数据结构和算法
- socket 通信，http 通信
- 数据库交互，查询，增删改，聚合、分析和处理
- 简单人工智能模型的理论理解和实践积累
- 还可以有：数据的可视化处理

整体上，就先这些吧。

对了，基本掌握 Python，并可以熟练使用。